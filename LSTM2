import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Cargar el dataset
file_path = r"C:\Users\josen\OneDrive\Desktop\Tesis\Solo2024.xlsx"
df = pd.read_excel(file_path, sheet_name="Hoja1", engine="openpyxl")


df["FechaPartida"] = pd.to_datetime(df["FechaPartida"], errors='coerce')
df.dropna(subset=["FechaPartida"], inplace=True)

# Obtener el **promedio de los 3 viajes con mayor ocupación por día**
df_max_ocupacion = df.groupby("FechaPartida").agg({
    "Ocupacion": lambda x: x.nlargest(3).mean(),
    "Butacas": "mean",
    "Kms": "mean",
    "Tipo de Cambio de Referencia - en Pesos - por Dolar": "mean"
}).reset_index()

# Normalizar los datos para mejorar el entrenamiento
scaler = MinMaxScaler(feature_range=(0,1))
df_scaled = scaler.fit_transform(df_max_ocupacion[["Ocupacion", "Butacas", "Kms", "Tipo de Cambio de Referencia - en Pesos - por Dolar"]])

#Crear secuencias de datos para LSTM
def create_sequences(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
        X.append(data[i:i+n_steps])
        y.append(data[i+n_steps, 0])  # La ocupación es la variable objetivo
    return np.array(X), np.array(y)


n_steps = 10
X, y = create_sequences(df_scaled, n_steps)

# Dividir en entrenamiento (80%) y prueba (20%)
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential([
    LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, X.shape[2])),
    Dropout(0.2),
    LSTM(100, activation='relu', return_sequences=True),
    Dropout(0.2),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(1)  # Capa de salida
])


# Compilar el modelo
model.compile(optimizer='adam', loss='mse')

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='mse')
history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), verbose=1)

# Hacer predicciones
y_pred = model.predict(X_test)

# Desnormalizar los valores para recuperar la escala original
y_test_inv = scaler.inverse_transform(np.hstack((y_test.reshape(-1,1), np.zeros((y_test.shape[0], 3)))))[:,0]
y_pred_inv = scaler.inverse_transform(np.hstack((y_pred, np.zeros((y_pred.shape[0], 3)))))[:,0]

# Calcular métricas de error del modelo LSTM
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)
r2 = r2_score(y_test_inv, y_pred_inv)

# Graficar comparación entre valores reales y predicciones
plt.figure(figsize=(12, 6))
plt.plot(df_max_ocupacion["FechaPartida"][train_size+n_steps:], y_test_inv, label="Real (Ocupación)", color="red")
plt.plot(df_max_ocupacion["FechaPartida"][train_size+n_steps:], y_pred_inv, label="Predicción LSTM", linestyle="dashed", color="green")
plt.legend()
plt.title("Modelo LSTM - Predicción de Ocupación")
plt.xlabel("Fecha de Partida")
plt.ylabel("Ocupación")
plt.grid()
plt.show()

# 🔹 Gráfico de pérdida
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.legend()
plt.title("Pérdida durante el Entrenamiento")
plt.xlabel("Épocas")
plt.ylabel("MSE")
plt.show()
    
# Mostrar métricas finales
print("\n Resultados del Modelo LSTM con Tipo de Cambio")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R² Score: {r2:.4f}")
