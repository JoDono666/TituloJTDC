import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Cargar el dataset
file_path = r"C:\Users\josen\OneDrive\Desktop\Tesis\Solo2024.xlsx"
df = pd.read_excel(file_path, sheet_name="Hoja1", engine="openpyxl")


df["FechaPartida"] = pd.to_datetime(df["FechaPartida"], errors='coerce')
df.dropna(subset=["FechaPartida"], inplace=True)

# Obtener el **promedio de los 3 viajes con mayor ocupaci贸n por d铆a**
df_max_ocupacion = df.groupby("FechaPartida").agg({
    "Ocupacion": lambda x: x.nlargest(3).mean(),
    "Butacas": "mean",
    "Kms": "mean",
    "Tipo de Cambio de Referencia - en Pesos - por Dolar": "mean"
}).reset_index()

# Normalizar los datos para mejorar el entrenamiento
scaler = MinMaxScaler(feature_range=(0,1))
df_scaled = scaler.fit_transform(df_max_ocupacion[["Ocupacion", "Butacas", "Kms", "Tipo de Cambio de Referencia - en Pesos - por Dolar"]])

#Crear secuencias de datos para LSTM
def create_sequences(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
        X.append(data[i:i+n_steps])
        y.append(data[i+n_steps, 0])  # La ocupaci贸n es la variable objetivo
    return np.array(X), np.array(y)


n_steps = 10
X, y = create_sequences(df_scaled, n_steps)

# Dividir en entrenamiento (80%) y prueba (20%)
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential([
    LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, X.shape[2])),
    Dropout(0.2),
    LSTM(100, activation='relu', return_sequences=True),
    Dropout(0.2),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(1)  # Capa de salida
])


# Compilar el modelo
model.compile(optimizer='adam', loss='mse')

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='mse')
history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), verbose=1)

# Hacer predicciones
y_pred = model.predict(X_test)

# Desnormalizar los valores para recuperar la escala original
y_test_inv = scaler.inverse_transform(np.hstack((y_test.reshape(-1,1), np.zeros((y_test.shape[0], 3)))))[:,0]
y_pred_inv = scaler.inverse_transform(np.hstack((y_pred, np.zeros((y_pred.shape[0], 3)))))[:,0]

# Calcular m茅tricas de error del modelo LSTM
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)
r2 = r2_score(y_test_inv, y_pred_inv)

# Graficar comparaci贸n entre valores reales y predicciones
plt.figure(figsize=(12, 6))
plt.plot(df_max_ocupacion["FechaPartida"][train_size+n_steps:], y_test_inv, label="Real (Ocupaci贸n)", color="red")
plt.plot(df_max_ocupacion["FechaPartida"][train_size+n_steps:], y_pred_inv, label="Predicci贸n LSTM", linestyle="dashed", color="green")
plt.legend()
plt.title("Modelo LSTM - Predicci贸n de Ocupaci贸n")
plt.xlabel("Fecha de Partida")
plt.ylabel("Ocupaci贸n")
plt.grid()
plt.show()

#  Gr谩fico de p茅rdida
plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Train Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.legend()
plt.title("P茅rdida durante el Entrenamiento")
plt.xlabel("pocas")
plt.ylabel("MSE")
plt.show()
    
# Mostrar m茅tricas finales
print("\n Resultados del Modelo LSTM con Tipo de Cambio")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R虏 Score: {r2:.4f}")
